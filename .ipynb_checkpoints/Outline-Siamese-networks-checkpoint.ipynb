{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Learning with Siamese Networks\n",
    "\n",
    "This is the jupyter notebook that accompanies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "All the imports are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from res_unet import SiameseNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Set of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img1, img2):\n",
    "    \n",
    "    img1 = TF.to_pil_image(img1, mode='L')\n",
    "    img2 = TF.to_pil_image(img2, mode='L')\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img1)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img2)\n",
    "    \n",
    "    plt.show()    \n",
    "    \n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class\n",
    "A simple class to manage configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"./data_madori/train/\"\n",
    "    testing_dir = \"./data_madori/test/\"\n",
    "    train_batch_size = 16\n",
    "    train_number_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "This dataset generates a pair of images. 0 for geniune pair and 1 for imposter pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MadoriSiameseDataset(Dataset):\n",
    "    def __init__(self, data_dir, img_size=(256, 256)):\n",
    "        self.img_paths = [os.path.join(Config.training_dir, x) for x in os.listdir(data_dir)]\n",
    "        self.img_size = (256, 256)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def _resize(self, img):\n",
    "        w, h = img.size\n",
    "        if w < h:\n",
    "            a = 256.0 / h\n",
    "            b = int(w * a)\n",
    "            img = img.resize((b, 256), Image.BILINEAR)\n",
    "        else:\n",
    "            a = 256.0 / w\n",
    "            b = int(h * a)\n",
    "            img = img.resize((256, b), Image.BILINEAR)\n",
    "        return img\n",
    "    \n",
    "    def _pad(self, img):\n",
    "        w, h = img.size\n",
    "        img = TF.pad(img, (0,0,256-w,0), padding_mode='edge') if h == 256 else \\\n",
    "               TF.pad(img, (0,0,0,256-h), padding_mode='edge')\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _transform(self, img):\n",
    "        return self._pad(self._resize(img))\n",
    "    \n",
    "    def _aug_img(self, image):\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.rotate(image, random.choice([90, 180, 270]))\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path1 = self.img_paths[idx]\n",
    "        img1 = self._transform(Image.open(img_path1).convert('L'))\n",
    "        label = random.randint(0, 1)\n",
    "        if label:\n",
    "            # choose different floorplan\n",
    "            img_path2 = img_path1\n",
    "            while img_path2 == img_path1:\n",
    "                img_path2 = random.choice(self.img_paths)\n",
    "            img2 = self._transform(Image.open(img_path2).convert('L'))\n",
    "        else:\n",
    "            # choose similar floorplan by augmentation\n",
    "            img2 = self._aug_img(img1)\n",
    "        img1, img2 = TF.to_tensor(img1), TF.to_tensor(img2)\n",
    "        return img1, img2, torch.from_numpy(np.array([label],dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising some of the data\n",
    "The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n",
    "1 indiciates dissimilar, and 0 indicates similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_loader = DataLoader(MadoriDataset(Config.training_dir), \n",
    "                           batch_size=Config.train_batch_size, \n",
    "                           shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(dset_loader):\n",
    "    img1, img2, label = batch\n",
    "    print(img1.size(), img2.size())\n",
    "    for k in range(3):\n",
    "        print(label[k])\n",
    "        imshow(img1[k], img2[k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - \n",
    "                                        euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(MadoriDataset(Config.training_dir), \n",
    "                                shuffle=True,\n",
    "                                batch_size=Config.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.0005 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(Config.train_number_epochs)):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        img0, img1, label = data\n",
    "        img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1,output2 = net(img0,img1)\n",
    "        loss_contrastive = criterion(output1,output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n",
    "    loss_history.append(loss_contrastive.item())\n",
    "#show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple testing\n",
    "The last 3 subjects were held out from the training, and will be used to test. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found more similar, while higher values indicate it found them to be dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                        transform=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset,num_workers=6,batch_size=1,shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "x0,_,_ = next(dataiter)\n",
    "\n",
    "for i in range(10):\n",
    "    _,x1,label2 = next(dataiter)\n",
    "    concatenated = torch.cat((x0,x1),0)\n",
    "    \n",
    "    output1,output2 = net(Variable(x0).to(device),Variable(x1).to(device))\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
