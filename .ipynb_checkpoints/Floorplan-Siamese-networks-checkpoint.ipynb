{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from res_unet import SiameseNetwork\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from kornia.losses import DiceLoss\n",
    "from losses import ContrastiveLoss\n",
    "from utils import *\n",
    "from datasets import MadoriOutlineDS, MadoriOutlineSiameseDS\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "from torch.optim import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = './data/image'\n",
    "label_dir = './data/outline'\n",
    "pair_madori_dir = './data/pair_madori'\n",
    "checkpoint_dir = './checkpoint'\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "\n",
    "train_file = './data/train.txt'\n",
    "val_file = './data/val.txt'\n",
    "test_file = './data/test.txt'\n",
    "pair_madori_file = './data/pair_madori.txt'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(MadoriOutlineSiameseDS(train_file), batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(MadoriOutlineSiameseDS(val_file), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dl):\n",
    "    img1, label1, img2, label2, is_diff = batch\n",
    "    for k in range(8):\n",
    "        print(is_diff[k])\n",
    "        imshow4(img1[k], label1[k], img2[k], label2[k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "criterion_unet = DiceLoss()\n",
    "criterion_siamese = ContrastiveLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_train_loss_history, unet_val_loss_history = [], []\n",
    "sia_train_loss_history, sia_val_loss_history = [], []\n",
    "unet_lowest_epoch_train_loss = unet_lowest_epoch_val_loss = float('inf')\n",
    "sia_lowest_epoch_train_loss = sia_lowest_epoch_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    unet_epoch_train_loss = sia_epoch_train_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        img1, label1, img2, label2, is_diff = batch\n",
    "        img1, label1 = img1.to(device), label1.to(device)\n",
    "        img2, label2 = img2.to(device), label2.to(device)\n",
    "        is_diff = is_diff.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output_unet1, output_unet2, output1, output2 = model(img1, img2)\n",
    "        \n",
    "        # unet loss\n",
    "        unet_batch_train_loss = criterion_unet(output_unet1, torch.squeeze(label1.long(), dim=1))\n",
    "        unet_batch_train_loss += criterion_unet(output_unet2, torch.squeeze(label2.long(), dim=1))\n",
    "        unet_epoch_train_loss += unet_batch_train_loss.item()\n",
    "        \n",
    "        # siamese loss\n",
    "        sia_batch_train_loss = criterion_siamese(output1, output2, is_diff)\n",
    "        sia_epoch_train_loss += sia_batch_train_loss.item()\n",
    "        \n",
    "        # optimize with total loss\n",
    "        total_loss = unet_batch_train_loss + sia_batch_train_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    unet_epoch_train_loss /= (i+1)\n",
    "    sia_epoch_train_loss /= (i+1)\n",
    "    if sia_epoch_train_loss < sia_lowest_epoch_train_loss:\n",
    "        sia_lowest_epoch_train_loss = sia_epoch_train_loss\n",
    "        torch.save(model.state_dict(), f'{checkpoint_dir}/best_train.pth')\n",
    "    unet_train_loss_history += [unet_epoch_train_loss]\n",
    "    sia_train_loss_history += [sia_epoch_train_loss]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        unet_epoch_val_loss = sia_epoch_val_loss = 0\n",
    "        for i, batch in enumerate(val_dl):\n",
    "            img1, label1, img2, label2, is_diff = batch\n",
    "            img1, label1 = img1.to(device), label1.to(device)\n",
    "            img2, label2 = img2.to(device), label2.to(device)\n",
    "            is_diff = is_diff.to(device)\n",
    "            \n",
    "            output_unet1, output_unet2, output1, output2 = model(img1, img2)\n",
    "            \n",
    "            # unet loss\n",
    "            unet_batch_val_loss = criterion_unet(output_unet1, torch.squeeze(label1.long(), dim=1))\n",
    "            unet_batch_val_loss += criterion_unet(output_unet2, torch.squeeze(label2.long(), dim=1))\n",
    "            unet_epoch_val_loss += unet_batch_val_loss.item()\n",
    "\n",
    "            # siamese loss\n",
    "            sia_batch_val_loss = criterion_siamese(output1, output2, is_diff)\n",
    "            sia_epoch_val_loss += sia_batch_val_loss.item()\n",
    "            \n",
    "        unet_epoch_val_loss /= (i+1)\n",
    "        sia_epoch_val_loss /= (i+1)\n",
    "        if sia_epoch_val_loss < sia_lowest_epoch_val_loss:\n",
    "            sia_lowest_epoch_val_loss = sia_epoch_val_loss\n",
    "            torch.save(model.state_dict(), f'{checkpoint_dir}/best_val.pth')\n",
    "        unet_val_loss_history.append(unet_epoch_val_loss)\n",
    "        sia_val_loss_history.append(sia_epoch_val_loss)\n",
    "        \n",
    "    print(f'Epoch {epoch} training unet/sia loss is {unet_epoch_train_loss}/{sia_epoch_train_loss}, \\\n",
    "          validation unet/sia loss is {unet_epoch_val_loss}/{sia_epoch_val_loss}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(train_loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet().to(device)\n",
    "model.load_state_dict(torch.load(f'{checkpoint_dir}/best_val.pth'))\n",
    "test_dl = DataLoader(MadoriOutlineDS(test_file), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_results_0 = []\n",
    "test_results_1 = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dl):\n",
    "        if i > 10: break\n",
    "        img, label = batch\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        pred = F.softmax(model(img), dim=1)\n",
    "        \n",
    "        img, label, pred = img.cpu(), label.cpu(), pred.cpu()\n",
    "        \n",
    "        ones = torch.ones((256, 256))\n",
    "        zeros = torch.zeros((256, 256))\n",
    "        \n",
    "        imshow(img[0], label[0], torch.where(pred[0,1] > 0.5, ones, zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pair test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet().to(device)\n",
    "model.load_state_dict(torch.load(f'{checkpoint_dir}/best_val.pth'))\n",
    "pair_test_dl = DataLoader(MadoriOutlineDS(pair_madori_file, pair_test=True), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, img in enumerate(pair_test_dl):\n",
    "        pred = F.softmax(model(img.to(device)), dim=1)\n",
    "        \n",
    "        img, pred = img.cpu(), pred.cpu()\n",
    "        \n",
    "        ones = torch.ones((256, 256))\n",
    "        zeros = torch.zeros((256, 256))\n",
    "        \n",
    "        imshow2(img[0], torch.where(pred[0,1] > 0.5, ones, zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
